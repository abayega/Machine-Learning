{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Phyla_VAE_3.ipynb","provenance":[{"file_id":"1_U-UKUsM33N9ggW_G5O04O0JR7_yyYt2","timestamp":1617303843958},{"file_id":"1naQFEd6ZGeDZOeZbX80LT-tP7FBUkRcI","timestamp":1613129176919},{"file_id":"1vQsY-BLEN5TElTqTC_0ePo0TaZ3sQbM_","timestamp":1611864428330}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"gqIyE3Y4uZpc"},"source":["# Introduction\n","\n","In this notebook you will find practical examples for every part of the theoretical part. Including how to build and train deep neural networks.\n","\n","You will also find questions that are designed to help you notice important concepts and implementation details."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OB7Wi3Ffyqbf","executionInfo":{"status":"ok","timestamp":1618313253554,"user_tz":240,"elapsed":3213,"user":{"displayName":"Anthony Bayega","photoUrl":"","userId":"02813902321244354985"}},"outputId":"80f96933-721f-4dd2-c058-aa40a97a9019"},"source":["import easydict\n","\n","##Calibrate\n","# pylint: disable=E1129,E0611,E1101\n","import argparse\n","import datetime\n","import random\n","from tqdm import tqdm\n","# Hide warnings before importing tf\n","import os\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n","#import tensorflow as tf\n","import pandas as pd\n","#from . import reformat\n","#from .load_data import split_features_labels, list_categorical_columns\n","#from .adjustments import Scaler, SigmoidScaler, split_discrete_continuous\n","#from .network import Confounded\n","\n","#Adjustments\n","import numpy as np\n","import pandas as pd\n","\n","#Network\n","# pylint: disable=E1129\n","!pip install tf-slim\n","import functools\n","import tensorflow.compat.v1 as tf\n","tf.disable_v2_behavior()\n","#import tensorflow as tf\n","#from tensorflow.contrib.layers import fully_connected, batch_norm # pylint: disable=E0611\n","from tf_slim.layers.layers import fully_connected, batch_norm\n","from math import ceil"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: tf-slim in /usr/local/lib/python3.7/dist-packages (1.1.0)\n","Requirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from tf-slim) (0.12.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.2.2->tf-slim) (1.15.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IQtuQHpkb5H8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618313258717,"user_tz":240,"elapsed":364,"user":{"displayName":"Anthony Bayega","photoUrl":"","userId":"02813902321244354985"}},"outputId":"e87e3e9b-4039-4f15-b3f5-e082d675b8fc"},"source":["#Load google drive\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6nBdtnUI08VU","executionInfo":{"status":"ok","timestamp":1618313263266,"user_tz":240,"elapsed":319,"user":{"displayName":"Anthony Bayega","photoUrl":"","userId":"02813902321244354985"}}},"source":["dirc = '/content/gdrive/MyDrive/AI in Microbiome/Filtered normalized data/noBE_removal_clr_and_other_normalizations/MAD/'\n","dirc = '/content/gdrive/MyDrive/PhD/Coursework/AI in Genomics/Projects/Phyla/phyla_dataset_d3/'\n","file1 = 'phyla_all_3949x1177_pmi_0_clr_75p_MAD.csv'\n","file2 = 'phyla_all_565x1177_pmi_0_clr_10p_MAD.csv'\n","file3 = 'phyla_all_753x1177_pmi_0_clr_15p_MAD.csv'\n","file4 = 'phyla_biopsy_1109x1177_pmi_0_clr_75p_MAD.csv'\n","file5 = 'phyla_biopsy_164x1177_pmi_0_clr_10p_MAD.csv'\n","file6 = 'phyla_biopsy_213x1177_pmi_0_clr_15p_MAD.csv'\n","file7 = 'phyla_stool_2840x1177_pmi_0_clr_75p_MAD.csv'\n","file8 = 'phyla_stool_401x1177_pmi_0_clr_10p_MAD.csv'\n","file9 = 'phyla_stool_540x1177_pmi_0_clr_15p_MAD.csv'\n","file10 = 'phyla_dataset_d3.csv'\n","\n","file_used = file10\n","#os.listdir(dirc)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"KKvMPCez5rWz","executionInfo":{"status":"ok","timestamp":1618313271784,"user_tz":240,"elapsed":3935,"user":{"displayName":"Anthony Bayega","photoUrl":"","userId":"02813902321244354985"}}},"source":["microbe_df = pd.read_csv(F'%s' %(dirc+file_used))"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":98},"id":"qatoHveU6YPn","executionInfo":{"status":"ok","timestamp":1618313274243,"user_tz":240,"elapsed":243,"user":{"displayName":"Anthony Bayega","photoUrl":"","userId":"02813902321244354985"}},"outputId":"49c6103b-d964-4d59-9f90-4299ab5b221b"},"source":["#Determine size of dataframe\n","print(microbe_df.shape)\n","microbe_df.iloc[1:2,1177:1184]"],"execution_count":7,"outputs":[{"output_type":"stream","text":["(5267, 1184)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unassigned;__;__;__;__;__</th>\n","      <th>col_site</th>\n","      <th>diagnosis</th>\n","      <th>sample_title</th>\n","      <th>stool_biopsy</th>\n","      <th>studyID</th>\n","      <th>uc_cd</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>0.0</td>\n","      <td>OSCCAR</td>\n","      <td>1</td>\n","      <td>1939.100003</td>\n","      <td>stool</td>\n","      <td>GEVERSM</td>\n","      <td>CD</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Unassigned;__;__;__;__;__ col_site  diagnosis  ... stool_biopsy  studyID uc_cd\n","1                        0.0   OSCCAR          1  ...        stool  GEVERSM    CD\n","\n","[1 rows x 7 columns]"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"PK6ogquV79jY"},"source":["#Subset df\n","#microbe_df = microbe_df.loc[microbe_df['uc_cd' ] == 'Control']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mihgtff68Swp","executionInfo":{"status":"ok","timestamp":1618313290754,"user_tz":240,"elapsed":284,"user":{"displayName":"Anthony Bayega","photoUrl":"","userId":"02813902321244354985"}}},"source":["#Rename columns and col_site\n","phenotype_df = microbe_df\n","phenotype_df.loc[(phenotype_df.col_site == 'AG'), 'col_site'] = 1\n","phenotype_df.loc[(phenotype_df.col_site == 'North Carolina'), 'col_site'] = 2\n","phenotype_df.loc[(phenotype_df.col_site == 'BOULDER'), 'col_site'] = 3\n","phenotype_df.loc[(phenotype_df.col_site == 'RISK'), 'col_site'] = 4\n","phenotype_df.loc[(phenotype_df.col_site == 'paneth_cells'), 'col_site'] = 5\n","phenotype_df.loc[(phenotype_df.col_site == 'CVDF'), 'col_site'] = 6\n","phenotype_df.loc[(phenotype_df.col_site == 'Cincinnati'), 'col_site'] = 7\n","phenotype_df.loc[(phenotype_df.col_site == 'MGH'), 'col_site'] = 8\n","phenotype_df.loc[(phenotype_df.col_site == 'PRISM'), 'col_site'] = 9\n","phenotype_df.loc[(phenotype_df.col_site == 'MGH Pediatrics'), 'col_site'] = 10\n","phenotype_df.loc[(phenotype_df.col_site == 'Cedars-Sinai'), 'col_site'] = 11\n","phenotype_df.loc[(phenotype_df.col_site == 'PRJNA436359'), 'col_site'] = 12\n","phenotype_df.loc[(phenotype_df.col_site == 'OSCCAR'), 'col_site'] = 13"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"CjyTf-fRAec0","executionInfo":{"status":"ok","timestamp":1618313294071,"user_tz":240,"elapsed":241,"user":{"displayName":"Anthony Bayega","photoUrl":"","userId":"02813902321244354985"}}},"source":["#Rename col_site\n","phenotype_df.rename(columns={'sample_title':'sample'}, inplace=True)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"tOUU7zCZBaoB","executionInfo":{"status":"ok","timestamp":1618313296008,"user_tz":240,"elapsed":582,"user":{"displayName":"Anthony Bayega","photoUrl":"","userId":"02813902321244354985"}}},"source":["#Change sample ids to a vector of integers\n","phenotype_df['sample'] = list(range(0,phenotype_df.shape[0],1))"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3wJS87cH9W-q","executionInfo":{"status":"ok","timestamp":1618313297481,"user_tz":240,"elapsed":276,"user":{"displayName":"Anthony Bayega","photoUrl":"","userId":"02813902321244354985"}},"outputId":"53af7a14-08b3-47df-b863-9213d0307120"},"source":["phenotype_df.col_site.value_counts()"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1     1279\n","4     1210\n","12    1178\n","2      516\n","3      467\n","9      260\n","13      91\n","5       82\n","7       55\n","8       45\n","6       39\n","11      25\n","10      20\n","Name: col_site, dtype: int64"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"rsotGIQVEC9H","executionInfo":{"status":"ok","timestamp":1618313309916,"user_tz":240,"elapsed":6339,"user":{"displayName":"Anthony Bayega","photoUrl":"","userId":"02813902321244354985"}}},"source":["phenotype_df.to_csv('/content/gdrive/MyDrive/PhD/Coursework/AI in Genomics/VAE/' + file_used.replace('.csv','_edited.csv'), index = False)"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"31bIVQNsFs1q","executionInfo":{"status":"ok","timestamp":1618313318538,"user_tz":240,"elapsed":286,"user":{"displayName":"Anthony Bayega","photoUrl":"","userId":"02813902321244354985"}}},"source":["#Read data file\n","input = '/content/gdrive/MyDrive/PhD/Coursework/AI in Genomics/VAE/' + file_used.replace('.csv','_edited.csv')\n","output = '/content/gdrive/MyDrive/PhD/Coursework/AI in Genomics/VAE/' + file_used.replace('.csv','_edited_vae.csv')\n","logfile = '/content/gdrive/MyDrive/PhD/Coursework/AI in Genomics/VAE/vae_log.csv'"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zecFPokz2LH_"},"source":["##Main: Calibrate\n","https://github.com/jdayton3/Confounded/blob/master/confounded/reformat.py"]},{"cell_type":"code","metadata":{"id":"5B9VhA5OyzxU","executionInfo":{"status":"ok","timestamp":1618313533497,"user_tz":240,"elapsed":284,"user":{"displayName":"Anthony Bayega","photoUrl":"","userId":"02813902321244354985"}}},"source":["args = easydict.EasyDict({\n","    \"file\" : input,           #Path to input file\n","    \"source_file\" : input,    #Path to input file\n","    \"output_file\" : output,   #Path to output file. (default: `source-file`_confounded.csv)\n","    \"minibatch_size\" : 100, #The size of the mini-batch for training. Must be positive integer.\"\n","    \"layers\" : 10,          #How many layers deep the discriminator should be. Must be positive integer.\n","    \"ae_layers\" : 2,        #How many layers in each of the encoding and decoding portions of the autoencoder.\n","    \"batch_col\" : \"col_site\",  #Which column contains the batch to adjust for\n","    \"code_size\" : 20,       #How many nodes in the code layer of the autoencoder.\"\n","    \"early_stopping\" : None, #type=positive_int, default=None, help=\"How many iterations without improvement before stopping early.\n","    \"scaling\" : \"linear\",   #choices=[\"linear\", \"sigmoid\"], default=\"linear\", help=\"Type of scaling to perform on the input data.\n","    \"loss_weight\" : 2.0,    #Weight applied to the discriminator loss when training the autoencoder.\n","    \"log_file\" : logfile, #Path to file to log results\n","    \"iterations\" : 10000,   #Number of iterations of minibatches to run\n","    \"save_model\" : None,    #Path to save the model weights. Weights are not saved if path is not specified\n","    \"load_model\" : None,    #help=\"Path to model weights checkpoint to load.\", \" Weights are initialized randomly if path is not specified.\")\n","    \"learning_rate\" : 0.0001, #, type=float, \n","})"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"hsmHPqsx2MWt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618314726837,"user_tz":240,"elapsed":1191130,"user":{"displayName":"Anthony Bayega","photoUrl":"","userId":"02813902321244354985"}},"outputId":"027f5601-ea0f-46d6-d94e-f9e399e90ab0"},"source":["\n","def parse_arguments():\n","    activation = tf.nn.relu\n","\n","    def positive_int(value):\n","        ivalue = int(value)\n","        if ivalue < 0:\n","            raise argparse.ArgumentTypeError(\"%s is an invalid positive int value\" % value)\n","        return ivalue\n","    '''\n","    parser = argparse.ArgumentParser(\n","        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n","    )\n","    parser.add_argument(\n","        'file', metavar='source-file', type=str,\n","        help='Path to input file.')\n","    parser.add_argument(\n","        '-o', \"--output-file\", type=str,\n","        help=\"Path to output file. (default: `source-file`_confounded.csv)\")\n","    parser.add_argument(\n","        \"-m\", \"--minibatch-size\", type=positive_int, default=100,\n","        help=\"The size of the mini-batch for training. Must be positive integer.\")\n","    parser.add_argument(\n","        \"-l\", \"--layers\", type=positive_int, default=10,\n","        help=\"How many layers deep the discriminator should be. Must be positive integer.\")\n","    parser.add_argument(\n","        \"-a\", \"--ae-layers\", type=positive_int, default=2,\n","        help=\"How many layers in each of the encoding and decoding portions of the autoencoder.\")\n","    parser.add_argument(\n","        \"-b\", \"--batch-col\", type=str, default=\"Batch\",\n","        help=\"Which column contains the batch to adjust for.\")\n","    parser.add_argument(\n","        \"-c\", \"--code-size\", type=positive_int, default=20,\n","        help=\"How many nodes in the code layer of the autoencoder.\")\n","    parser.add_argument(\n","        \"-e\", \"--early-stopping\", type=positive_int, default=None,\n","        help=\"How many iterations without improvement before stopping early.\")\n","    parser.add_argument(\n","        \"-s\", \"--scaling\", choices=[\"linear\", \"sigmoid\"], default=\"linear\",\n","        help=\"Type of scaling to perform on the input data.\")\n","    parser.add_argument(\n","        \"-w\", \"--loss-weight\", type=float, default=1.0,\n","        help=\"Weight applied to the discriminator loss when training the autoencoder.\")\n","    parser.add_argument(\n","        \"-f\", \"--log-file\", type=str, default=\"./data/metrics/log.csv\",\n","        help=\"Path to file to log results.\")\n","    parser.add_argument(\n","        \"-i\", \"--iterations\", type=positive_int, default=10000,\n","        help=\"Number of iterations of minibatches to run.\")\n","    parser.add_argument(\n","        \"-d\", \"--save-model\", type=str, default=None,\n","        help=\"Path to save the model weights. Weights are not saved if path is not specified.\")\n","    parser.add_argument(\n","        \"-r\", \"--load-model\", type=str, default=None,\n","        help=\"Path to model weights checkpoint to load.\"\n","            \" Weights are initialized randomly if path is not specified.\")\n","    parser.add_argument(\n","        \"-g\", \"--learning-rate\", type=float, default=0.0001,\n","        help=\"Global learning rate for all portions of the network.\")\n","\n","    args = parser.parse_args()\n","    '''\n","    if not args.output_file:\n","        args.output_file = args.file.rstrip(\".csv\") + \"_confounded.csv\"\n","    args.activation = activation\n","\n","    return args\n","\n","def now():\n","    \"\"\"Return the current time in iso format\"\"\"\n","    return datetime.datetime.now().isoformat()\n","\n","def should_train_dual(i, tot_iterations):\n","    \"\"\"For a given iteration, should we train ae & disc?\n","    Cools down over time.\n","    \"\"\"\n","    probability = 1.0 - i / tot_iterations\n","    probability = 0.1\n","    return random.uniform(0.0, 1.0) < probability\n","\n","class SummaryLogger(object):\n","    def __init__(self,\n","                 log_file,\n","                 input_path,\n","                 output_path,\n","                 code_size,\n","                 d_layers,\n","                 ae_layers,\n","                 minibatch_size,\n","                 activation,\n","                 batch_col,\n","                 scaling,\n","                 loss_weight):\n","        self.start_time = now()\n","        self.log_file = log_file\n","        self.input_path = input_path\n","        self.output_path = output_path\n","        self.code_size = code_size\n","        self.d_layers = d_layers\n","        self.ae_layers = ae_layers\n","        self.minibatch_size = minibatch_size\n","        self.activation = activation\n","        self.batch_col = batch_col\n","        self.scaling = scaling\n","        self.loss_weight = loss_weight\n","        self.values = {\n","            \"start_time\": [],\n","            \"input_path\": [],\n","            \"output_path\": [],\n","            \"batch_column\": [],\n","            \"minibatch_size\": [],\n","            \"code_size\": [],\n","            \"discriminator_layers\": [],\n","            \"autoencoder_layers\": [],\n","            \"activation\": [],\n","            \"scaling_method\": [],\n","            \"loss_weight\": [],\n","            \"time\": [],\n","            \"iteration\": [],\n","            \"ae_loss\": [],\n","            \"disc_loss\": [],\n","            \"dual_loss\": [],\n","        }\n","\n","    def log(self, iteration, ae_loss, disc_loss, dual_loss):\n","        self.values[\"start_time\"].append(self.start_time)\n","        self.values[\"input_path\"].append(self.input_path)\n","        self.values[\"output_path\"].append(self.output_path)\n","        self.values[\"batch_column\"].append(self.batch_col)\n","        self.values[\"minibatch_size\"].append(self.minibatch_size)\n","        self.values[\"code_size\"].append(self.code_size)\n","        self.values[\"discriminator_layers\"].append(self.d_layers)\n","        self.values[\"autoencoder_layers\"].append(self.ae_layers)\n","        self.values[\"activation\"].append(\".\".join([\n","            self.activation.__module__,\n","            self.activation.__name__\n","        ]))\n","        self.values[\"scaling_method\"].append(self.scaling)\n","        self.values[\"loss_weight\"].append(self.loss_weight)\n","        self.values[\"time\"].append(now())\n","        self.values[\"iteration\"].append(iteration)\n","        self.values[\"ae_loss\"].append(ae_loss)\n","        self.values[\"disc_loss\"].append(disc_loss)\n","        self.values[\"dual_loss\"].append(dual_loss)\n","\n","    def save(self):\n","        new_df = pd.DataFrame(self.values)\n","        old_df = None\n","        try:\n","            old_df = pd.read_csv(self.log_file)\n","        except:\n","            pass\n","        if old_df is not None:\n","            new_df = pd.concat([old_df, new_df], ignore_index=True, sort=False)\n","        new_df.to_csv(self.log_file, index=False)\n","\n","def autoencoder(input_path,\n","                output_path,\n","                save_weights_path,\n","                load_weights_path,\n","                minibatch_size=100,\n","                code_size=200,\n","                iterations=10000,\n","                d_layers=2,\n","                ae_layers=2,\n","                activation=tf.nn.relu,\n","                batch_col=\"Batch\",\n","                early_stopping=None,\n","                scaling=\"linear\",\n","                disc_weighting=1.0,\n","                log_file=\"log.csv\",\n","                learning_rate=0.0001):\n","\n","    # Get sizes & meta cols\n","    print(\"Loading data\")\n","    data = pd.read_csv(input_path)\n","    original_cols = data.columns.tolist()\n","\n","    scaling_options = {\n","        \"linear\": Scaler,\n","        \"sigmoid\": SigmoidScaler\n","    }\n","\n","    scaler = scaling_options[scaling]()\n","    data = scaler.squash(data)\n","\n","    meta_cols = list_categorical_columns(data)\n","    print(\"Inferred meta columns: {}. These will not be adjusted.\".format(meta_cols))\n","\n","    input_size = len(data.columns) - len(meta_cols)\n","    num_targets = len(data[batch_col].unique())\n","\n","    c = Confounded(\n","        input_size,\n","        code_size,\n","        num_targets,\n","        discriminator_layers=d_layers,\n","        autoencoder_layers=ae_layers,\n","        activation=activation,\n","        disc_weghting=disc_weighting\n","    )\n","\n","    all_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n","    vars_to_replace = [var for var in all_vars if \"do_not_save\" in var.name]\n","    vars_to_keep = [var for var in all_vars if var not in vars_to_replace]\n","    saver = tf.train.Saver(vars_to_keep)\n","\n","    with tf.Session() as sess:\n","        if load_weights_path:\n","            saver.restore(sess, load_weights_path)\n","            print(\"Model loaded from path: {}\".format(load_weights_path))\n","            tf.variables_initializer(vars_to_replace).run()\n","        else:\n","            tf.global_variables_initializer().run()\n","        merged = tf.summary.merge_all()\n","        writer = tf.summary.FileWriter(\"log/{}\".format(now()), sess.graph)\n","\n","        logger = SummaryLogger(\n","            log_file,\n","            input_path,\n","            output_path,\n","            code_size,\n","            d_layers,\n","            ae_layers,\n","            minibatch_size,\n","            activation,\n","            batch_col,\n","            scaling,\n","            disc_weighting\n","        )\n","        # Train\n","        n_since_improvement = 0\n","        best_loss = float(\"inf\")\n","\n","        print(\"Training Confounded (learning rate={})\".format(learning_rate))\n","        for i in tqdm(range(iterations)):\n","            features, labels = split_features_labels(\n","                data,\n","                batch_col,\n","                meta_cols=meta_cols,\n","                sample=minibatch_size\n","            )\n","            summary, disc_loss, ae_loss, dual_loss, _, _, _ = sess.run([\n","                merged,\n","                c.d_loss,\n","                c.ae_loss,\n","                c.loss,\n","                c.outputs,\n","                c.optimizer,\n","                c.d_optimizer,\n","            ], feed_dict={\n","                c.inputs: features,\n","                c.targets: labels,\n","            })\n","\n","            writer.add_summary(summary, i)\n","            logger.log(i, ae_loss, disc_loss, dual_loss)\n","            if dual_loss < best_loss:\n","                n_since_improvement = 0\n","                best_loss = dual_loss\n","            else:\n","                n_since_improvement += 1\n","            if early_stopping is not None and n_since_improvement >= early_stopping:\n","                print(\"No loss improvement for {} iterations. Stopping at iteration {}.\".format(\n","                    n_since_improvement,\n","                    i\n","                ))\n","                break\n","\n","        logger.save()\n","        if save_weights_path:\n","            saver.save(sess, save_weights_path)\n","            print(\"Model saved in path: {}\".format(save_weights_path))\n","\n","        print(\"Adjusting the input data\")\n","        features, labels = split_features_labels(data, batch_col, meta_cols=meta_cols)\n","        adj, = sess.run([c.outputs], feed_dict={\n","            c.inputs: features,\n","            c.targets: labels,\n","        })\n","        print(\"Saving data to {}\".format(output_path))\n","        df_adj = pd.DataFrame(adj, columns=split_discrete_continuous(data)[-1].columns)\n","        df_adj = scaler.unsquash(df_adj)\n","\n","        to_csv(\n","            df_adj,\n","            output_path,\n","            tidy=True,\n","            meta_cols={\n","                col: data[col] for col in meta_cols\n","            },\n","            column_order=original_cols\n","        )\n","\n","if __name__ == \"__main__\":\n","\n","    args = parse_arguments()\n","\n","    autoencoder(\n","        args.file,\n","        args.output_file,\n","        args.save_model,\n","        args.load_model,\n","        args.minibatch_size,\n","        args.code_size,\n","        args.iterations,\n","        d_layers=args.layers,\n","        ae_layers=args.ae_layers,\n","        activation=args.activation,\n","        batch_col=args.batch_col,\n","        early_stopping=args.early_stopping,\n","        scaling=args.scaling,\n","        disc_weighting=args.loss_weight,\n","        log_file=args.log_file,\n","        learning_rate=args.learning_rate\n","    )"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Loading data\n","Inferred meta columns: ['run_accession', 'col_site', 'diagnosis', 'sample', 'stool_biopsy', 'studyID', 'uc_cd']. These will not be adjusted.\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:201: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1719: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n","  warnings.warn('`layer.apply` is deprecated and '\n","  0%|          | 0/10000 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training Confounded (learning rate=0.0001)\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 10000/10000 [19:30<00:00,  8.54it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Adjusting the input data\n","Saving data to /content/gdrive/MyDrive/PhD/Coursework/AI in Genomics/VAE/phyla_dataset_d3_edited_vae.csv\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"uZofGygQ2hln"},"source":["##Adjustments"]},{"cell_type":"code","metadata":{"id":"JYM4H7hd2kQK","executionInfo":{"status":"ok","timestamp":1618313337669,"user_tz":240,"elapsed":382,"user":{"displayName":"Anthony Bayega","photoUrl":"","userId":"02813902321244354985"}}},"source":["\"\"\"Adjustments to dataframes\"\"\"\n","\n","def split_discrete_continuous(df):\n","    discrete_types = ['object', 'int']\n","    discrete = df.select_dtypes(include=discrete_types)\n","    continuous = df.select_dtypes(exclude=discrete_types)\n","    return discrete, continuous\n","\n","class Scaler(object):\n","    \"\"\"Scale or unscale a dataframe from [min, max] <-> [0, 1]\n","    \"\"\"\n","    def __init__(self):\n","        self.col_min = None\n","        self.col_max = None\n","        self.squashed = False\n","\n","    def squash(self, df):\n","        \"\"\"Adjust the dataframe to the [0, 1] range.\n","        Arguments:\n","            df {pandas.DataFrame} -- The quantitative dataframe to be\n","                squashed.\n","        Returns:\n","            pandas.DataFrame -- The squashed dataframe.\n","        \"\"\"\n","        discrete, continuous = split_discrete_continuous(df)\n","        scaled = self._squash_continuous(continuous)\n","        self.squashed = True\n","        return pd.concat([discrete, scaled], axis=\"columns\")\n","\n","    def _squash_continuous(self, continuous):\n","        self.col_min = continuous.min()\n","        self.col_max = continuous.max()\n","        already_in_range = (\n","            (self.col_min >= 0.0) &\n","            (self.col_min <= 1.0) &\n","            (self.col_max >= 0.0) &\n","            (self.col_max <= 1.0)\n","        )\n","        self.col_min = np.where(already_in_range, 0.0, self.col_min)\n","        self.col_max = np.where(already_in_range, 1.0, self.col_max)\n","\n","        scaled = (continuous - self.col_min) / (self.col_max - self.col_min)\n","        return scaled\n","\n","    def unsquash(self, df):\n","        \"\"\"Adjust the dataframe back to the original range.\n","        Arguments:\n","            df {pandas.DataFrame} -- The quantitative dataframe to be\n","                expanded.\n","        Returns:\n","            pandas.DataFrame -- The dataframe with each column expanded\n","                to its original range.\n","        \"\"\"\n","        if not self.squashed:\n","            raise Exception(\n","                \"Error: Scaler.squash() must be run \"\n","                \"before Scaler.unsquash() can be used.\"\n","            )\n","        discrete, continuous = split_discrete_continuous(df)\n","        scaled = self._unsquash_continuous(continuous)\n","        if discrete.size == 0:\n","            return scaled\n","        return pd.concat([discrete, scaled], axis=\"columns\")\n","\n","    def _unsquash_continuous(self, continuous):\n","        scaled = continuous * (self.col_max - self.col_min) + self.col_min\n","        return scaled\n","\n","class SigmoidScaler(Scaler):\n","    \"\"\"Scaler class that scales continuous values into (0.0, 1.0) using\n","    the sigmoid function.\n","    \"\"\"\n","    def _squash_continuous(self, continuous):\n","        return self.__sigmoid(continuous)\n","\n","    def _unsquash_continuous(self, continuous):\n","        return self.__logit(continuous)\n","\n","    def __sigmoid(self, continuous):\n","        return 1.0 / (1.0 + np.exp(-continuous))\n","\n","    def __logit(self, continuous):\n","        \"\"\"inverse of sigmoid\"\"\"\n","        return np.log(continuous) - np.log(1 - continuous)\n"],"execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K61q4Nuq19lw"},"source":["##Load data"]},{"cell_type":"code","metadata":{"id":"EOAivHy72ABg","executionInfo":{"status":"ok","timestamp":1618313335694,"user_tz":240,"elapsed":454,"user":{"displayName":"Anthony Bayega","photoUrl":"","userId":"02813902321244354985"}}},"source":["\n","def split_features_labels(df, batch_col, meta_cols=None, sample=None):\n","    \"\"\"Split a dataframe into features and labels numpy arrays.\n","    Arguments:\n","        df {pandas.DataFrame} -- A tidy dataframe with meta columns, a\n","            Batch column, and quantitative data.\n","    Keyword Arguments:\n","        meta_cols {list of strings} -- Columns that should not be\n","            used as features to be batch-adjusted (default:\n","            {[\"Sample\", `batch_col`]})\n","        sample {int} -- The number of rows to sample. If None, return\n","            all rows. (default: {None})\n","    Returns:\n","        [(numpy.array, numpy.array)] -- Tuple of features and labels,\n","            where features are the quantitative data from the given\n","            dataframe and labels are the one-hot encoded batches for\n","            each instance.\n","    \"\"\"\n","    if meta_cols is None:\n","        meta_cols = [\"Sample\", batch_col]\n","    features = np.array(df.drop(meta_cols, axis=1))\n","    labels = pd.get_dummies(df[batch_col])\n","    labels = np.array(labels, dtype=float)\n","    if sample is not None:\n","        rows = pd.DataFrame(df.index).sample(sample, replace=True)[0].values.tolist()\n","        features = features[rows]\n","        labels = labels[rows]\n","    return features, labels\n","\n","def list_categorical_columns(df):\n","    \"\"\"Get the names of all categorical columns in the dataframe.\n","    Arguments:\n","        df {pandas.DataFrame} -- The dataframe.\n","    Returns:\n","        list -- Names of the categorical columns in the dataframe.\n","    \"\"\"\n","    return list(df.select_dtypes(exclude=['float']).columns)"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NH-udDF_12EJ"},"source":["##Network"]},{"cell_type":"code","metadata":{"id":"KMpGHym-13fS","executionInfo":{"status":"ok","timestamp":1618313332936,"user_tz":240,"elapsed":644,"user":{"displayName":"Anthony Bayega","photoUrl":"","userId":"02813902321244354985"}}},"source":["\"\"\"Definitions for the neural networks in Confounded.\n","\"\"\"\n","\n","def is_square(n):\n","    sqrt = n**0.5\n","    return int(sqrt) == sqrt\n","\n","def var_scope(scope):\n","    \"\"\"Decorator to wrap a function in a tensorflow variable scope\n","    Arguments:\n","        scope {str} -- Name of the variable scope\n","    Returns:\n","        function -- The decorated function wrapped in the variable scope\n","    \"\"\"\n","    def decorator_var_scope(func):\n","        @functools.wraps(func)\n","        def wrapper_var_scope(*args, **kwargs):\n","            with tf.variable_scope(scope):\n","                return func(*args, **kwargs)\n","        return wrapper_var_scope\n","    return decorator_var_scope\n","\n","def show_before_and_after_images(func):\n","    @functools.wraps(func)\n","    def wrapper_show_images(*args, **kwargs):\n","        inputs = args[0]\n","        show_image(inputs, name=\"inputs\")\n","        outputs = func(*args, **kwargs)\n","        if isinstance(outputs, tuple):\n","            # The autoencoder functions might return (outputs, loss)\n","            show_image(outputs[0], name=\"outputs\")\n","        else:\n","            show_image(outputs, name=\"outputs\")\n","        return outputs\n","    return wrapper_show_images\n","\n","@var_scope(\"vae\")\n","@show_before_and_after_images\n","def variational_autoencoder(inputs, code_size=20):\n","    \"\"\"Creates a variational autoencoder based on \"Hands-On Machine\n","    Learning with Scikit-Learn and TensorFlow by Aurélien Géron\n","    (O’Reilly). Copyright 2017 Aurélien Géron, 978-1-491-96229-9.\"\n","    Arguments:\n","        input_size {int} -- Size of the input to the autoencoder\n","    Returns:\n","        input {Tensor} -- The input tensor\n","        output {Tensor} -- The output tensor\n","        loss {Tensor} -- The loss operation\n","    \"\"\"\n","    layer_sizes = [500, 500]\n","    activations = [tf.nn.elu for _ in layer_sizes]\n","    input_size = get_layer_size(inputs)\n","\n","    encoding = make_layers(inputs, layer_sizes, activations)\n","    code_mean, code_gamma, code = vae_code_layer(encoding, code_size)\n","    decoding = make_layers(code, layer_sizes, activations)\n","    logits = fully_connected(decoding, input_size, activation_fn=None)\n","    outputs = tf.sigmoid(logits)\n","\n","    reconstruction_loss = tf.losses.mean_squared_error(inputs, outputs)\n","    xentropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=inputs, logits=logits)\n","    reconstruction_loss = tf.reduce_sum(xentropy)\n","    latent_loss = kl_divergence(code_gamma, code_mean) #* 0.01 / code_size\n","    loss = reconstruction_loss + latent_loss\n","\n","    loss = loss / input_size\n","\n","    return outputs, loss\n","\n","def get_layer_size(layer):\n","    dimensions = layer.shape[1:]\n","    size = 1\n","    for dimension in dimensions:\n","        size *= int(dimension) # must be converted from Dimension to int\n","    return size\n","\n","def make_layers(inputs, layer_sizes, activations=None, keep_prob=1.0, do_batch_norm=False):\n","    if not activations:\n","        activations = [tf.nn.relu for _ in layer_sizes]\n","    current_layer = inputs\n","    for layer_size, activation in zip(layer_sizes, activations):\n","        current_layer = fully_connected(current_layer, layer_size, activation_fn=activation)\n","        current_layer = tf.nn.dropout(current_layer, keep_prob)\n","        if do_batch_norm:\n","            current_layer = batch_norm(current_layer)\n","    return current_layer\n","\n","def vae_code_layer(inputs, code_size):\n","    code_mean = fully_connected(inputs, code_size, activation_fn=None)\n","    code_gamma = fully_connected(inputs, code_size, activation_fn=None)\n","    noise = tf.random_normal(tf.shape(code_gamma), dtype=tf.float32)\n","    code = code_mean + tf.exp(0.5 * code_gamma) * noise\n","    return code_mean, code_gamma, code\n","\n","def kl_divergence(gamma, mean):\n","    return 0.5 * tf.reduce_sum(tf.exp(gamma) + tf.square(mean) - 1 - gamma)\n","\n","def show_image(x, name=\"image\"):\n","    input_size = get_layer_size(x)\n","    if is_square(input_size):\n","        width_height = int(input_size**0.5)\n","        img = tf.reshape(x, [-1, width_height, width_height, 1])\n","        tf.summary.image(name, img, max_outputs=1)\n","\n","class Confounded(object):\n","    def __init__(self,\n","                 input_size,\n","                 code_size,\n","                 num_targets,\n","                 discriminator_layers=2,\n","                 autoencoder_layers=2,\n","                 activation=tf.nn.relu,\n","                 disc_weghting=1.0,\n","                 learning_rate=0.0001):\n","        self.sess = tf.Session() #tf.compat.v1.Session() #tf.Session()\n","\n","        self.input_size = input_size\n","        self.code_size = code_size\n","        self.num_targets = num_targets\n","        self.discriminator_layers = discriminator_layers\n","        self.autoencoder_layers = autoencoder_layers\n","        self.activation = activation\n","        self.disc_weighting = disc_weghting\n","        self.learning_rate = learning_rate\n","\n","        self.inputs = None\n","        self.code = None\n","        self.outputs = None\n","        self.targets = None\n","        self.logits = None\n","        self.classification = None\n","\n","        self.d_loss = None\n","        self.ae_loss = None\n","        self.loss = None\n","        self.optimizer = None\n","        self.d_optimizer = None\n","\n","        self._setup_networks()\n","\n","    def _setup_networks(self):\n","        self._setup_autoencoder()\n","        self._setup_discriminator()\n","        self._setup_loss_functions()\n","\n","    @var_scope(\"autoencoder\")\n","    def _setup_autoencoder(self):\n","        self.inputs = tf.placeholder(tf.float32, [None, self.input_size])\n","        self.outputs, self.ae_loss = variational_autoencoder(self.inputs, code_size=self.code_size)\n","\n","    @var_scope(\"discriminator\")\n","    def _setup_discriminator(self):\n","        self.targets = tf.placeholder(tf.float32, [None, self.num_targets])\n","        inputs = batch_norm(self.outputs)\n","        layer_size = 512\n","        layer_sizes = [int(ceil(layer_size / 2**n)) for n in range(self.discriminator_layers)]\n","        layer_sizes = [1024, 512, 512, 128]\n","        penultimate_layer = make_layers(self.outputs, layer_sizes, keep_prob=0.5, do_batch_norm=True)\n","        with tf.variable_scope(\"do_not_save\"):\n","            self.logits = fully_connected(penultimate_layer, self.num_targets, activation_fn=None)\n","            self.classification = tf.nn.sigmoid(self.logits)\n","\n","    @var_scope(\"discriminator\")\n","    @var_scope(\"optimizer\")\n","    def _setup_disc_loss(self):\n","        xentropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=self.targets, logits=self.logits)\n","        self.d_loss = tf.reduce_sum(xentropy) / self.num_targets\n","        tf.summary.scalar(\"d_loss\", self.d_loss)\n","        discriminator_vars = tf.get_collection(\n","            tf.GraphKeys.TRAINABLE_VARIABLES,\n","            \"discriminator\"\n","        )\n","        self.d_optimizer = tf.train.AdamOptimizer(\n","            learning_rate=self.learning_rate,\n","            name=\"discriminator\"\n","        ).minimize(self.d_loss, var_list=discriminator_vars)\n","\n","    @var_scope(\"autoencoder\")\n","    @var_scope(\"optimizer\")\n","    def _setup_ae_loss(self):\n","        tf.summary.scalar(\"ae_loss\", self.ae_loss)\n","        autoencoder_vars = tf.get_collection(\n","            tf.GraphKeys.TRAINABLE_VARIABLES,\n","            \"autoencoder\"\n","        )\n","        self.ae_optimizer = tf.train.AdamOptimizer(\n","            learning_rate=self.learning_rate,\n","            name=\"ae\"\n","        ).minimize(self.ae_loss, var_list=autoencoder_vars)\n","\n","    @var_scope(\"autoencoder\")\n","    @var_scope(\"optimizer\")\n","    def _setup_dual_loss(self):\n","        autoencoder_vars = tf.get_collection(\n","            tf.GraphKeys.TRAINABLE_VARIABLES,\n","            \"autoencoder\"\n","        )\n","        self.loss = self.ae_loss - self.disc_weighting * self.d_loss\n","        tf.summary.scalar(\"dual_loss\", self.loss)\n","        self.optimizer = tf.train.AdamOptimizer(\n","            learning_rate=self.learning_rate,\n","            name=\"dual\"\n","        ).minimize(self.loss, var_list=autoencoder_vars)\n","\n","    def _setup_loss_functions(self):\n","        self._setup_disc_loss()\n","        self._setup_ae_loss()\n","        self._setup_dual_loss()"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ppkDQiu31qZd"},"source":["##Reformat"]},{"cell_type":"code","metadata":{"id":"eVLCUben1tbg","executionInfo":{"status":"ok","timestamp":1618313329928,"user_tz":240,"elapsed":273,"user":{"displayName":"Anthony Bayega","photoUrl":"","userId":"02813902321244354985"}}},"source":["\n","def to_csv(df, path, tidy=False, meta_cols=None, column_order=None):\n","    \"\"\"Save a dataframe as a CSV, and optionally add meta columns and\n","        transpose the dataframe.\n","    Arguments:\n","        df {pandas.DataFrame} -- The data to be saved.\n","        path {str} -- The path where the csv should be saved.\n","    Keyword Arguments:\n","        tidy {bool} -- True if the rows are instances and the columns\n","            are features; False otherwise. (default: {False})\n","        meta_cols {dict} -- Dictionary of {column_name: [values, ...]}\n","            (default: {None})\n","    \"\"\"\n","    if not tidy:\n","        df = df.T\n","    df = _add_meta_cols(df, meta_cols)\n","\n","    if column_order:\n","        df = df[column_order]\n","\n","    df.to_csv(path, index=None)\n","\n","def _add_meta_cols(df, meta_cols):\n","    if meta_cols is None:\n","        meta_cols = {}\n","    sample = meta_cols.get(\"Sample\")\n","    df['Sample'] = _check_sample(df, sample)\n","    for col_name, col in meta_cols.items():\n","        if col_name in [\"Sample\"]:\n","            continue\n","        df[col_name] = col\n","    return _reorder_cols(df, list(meta_cols.keys()))\n","\n","def _check_sample(df, sample):\n","    if sample is None:\n","        return df.index\n","    return sample\n","\n","def _reorder_cols(df, meta_cols):\n","    cols = df.columns.tolist()\n","    for col in meta_cols:\n","        cols.remove(col)\n","    cols = meta_cols + cols\n","    return df[cols]\n","\n","if __name__ == \"__main__\":\n","    import functools\n","    import operator\n","    '''\n","    in_path = \"data/mnist_matrix.csv\"\n","    out_path = \"data/tidy_mnist2.csv\"\n","    df = pd.read_csv(in_path, header=None)\n","    to_csv(df, out_path, tidy=False, meta_cols={\n","        \"Batch\": None,\n","        \"Sample\": None,\n","        \"Digit\": functools.reduce(\n","            operator.add,\n","            [[x] * 1000 for x in range(10)]\n","        )\n","    })\n","    '''"],"execution_count":15,"outputs":[]}]}